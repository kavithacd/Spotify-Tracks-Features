#Importing standard libraries
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
from keras.models import Sequential
#from apyori import apriori
from keras.layers import Dense
import matplotlib.pyplot as plt
#List of variables
Data=pd.read_csv('SpotifyFeatures.csv')
#Column names
Data.keys()
#Checking for null values
Data.isnull().sum()
#No null values
#Dropping track ID as it does not affect the popularity
Data=Data.drop('track_id',axis=1)
#List of various genres
Data['genre'].unique()
#List of various artists
Data['artist_name'].unique()
#Distrubutions of all numerical variables
fig = plt.figure(figsize = (15,15))
ax = fig.gca()
Data.hist(ax = ax)
#Checking for patterns in data with categorical variables
#Popularity by key
plt.gcf().set_size_inches(8,6)
sns.boxplot(Data['popularity'],Data['key'])
#Though the spreads are similar, songs in the keys C#,G# and B seem to be slightly more popular
plt.gcf().set_size_inches(16, 10)
sns.boxplot(Data['popularity'],Data['genre'])
#Clear distinction on which genres are very popular
#Top 3 genres: Pop,Rock,Rap (closely followed by Dance and Hip-Hop)
#Popularity based on mode
sns.boxplot(Data['mode'],Data['popularity'])
#Popularity based on time signature
sns.boxplot(Data['time_signature'],Data['popularity'])
#Time signature zero varies a lot, but has almost no outliers
#Time signature 4 seems to have more popularity
#Encoding artist names
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
Data['artist_name']=le.fit_transform(Data['artist_name'])
#Checking for correlation in numerical variables
plt.gcf().set_size_inches(16, 10)
sns.pairplot(Data)
#Correlation between input variables
Data.drop('popularity',axis=1).corr()
#Significant correlation between: loudness and acousticness (neg), energy and acousticness (neg), danceability and valence (pos), energy and loudness (pos), instrumentalness and loudness (neg), speechiness and liveliness (pos)
#For use across all models with ease, encoding all categorical variables
le1=LabelEncoder()
Data['genre']=le1.fit_transform(Data['genre'])
le2=LabelEncoder()
Data['track_name']=le2.fit_transform(Data['track_name'])
le3=LabelEncoder()
Data['key']=le3.fit_transform(Data['key'])
le4=LabelEncoder()
Data['mode']=le4.fit_transform(Data['mode'])
le5=LabelEncoder()
Data['time_signature']=le5.fit_transform(Data['time_signature'])
#Creating train and test datasets
x=Data.drop('popularity',axis=1)
y=Data['popularity']
from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.25,random_state=7)
Data_num=Data.drop(['genre','artist_name','track_name','key','mode','time_signature'],axis=1)
Data_num.head()
#['acousticness','danceability','duration_ms','energy','instrumentalness','liveliness','loudness','speechiness','tempo','valence']
#For use across all models with ease, scaling all numerical data using zscore
from scipy.stats import zscore
scaled_num=Data_num.apply(zscore)
#Scaling all numerical features for the benefit of neural network training
Data['acousticness']=scaled_num['acousticness']
Data['danceability']=scaled_num['danceability']
Data['duration_ms']=scaled_num['duration_ms']
Data['energy']=scaled_num['energy']
Data['instrumentalness']=scaled_num['instrumentalness']
Data['liveness']=scaled_num['liveness']
Data['loudness']=scaled_num['loudness']
Data['speechiness']=scaled_num['speechiness']
Data['tempo']=scaled_num['tempo']
Data['valence']=scaled_num['valence']
#MODEL BUILDING
#Linear Regression
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
l=LinearRegression()
lr=l.fit(xtrain,ytrain)
scores = cross_val_score(lr, xtrain, ytrain, cv=10)
predicted=lr.predict(xtest)
print("Cross-validation scores for Random Forest Regressor are: ", scores)
#Ensemble methods
#Random Forest
from sklearn.ensemble import RandomForestRegressor
rf=RandomForestRegressor(n_estimators=100,min_samples_leaf=25)
rfr=rf.fit(xtrain,ytrain)
scores = cross_val_score(rf, xtrain, ytrain, cv=10)
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
#Feature importance from Random Forest
importances = rfr.feature_importances_
std = np.std([tree.feature_importances_ for tree in rfr.estimators_],
             axis=0)
indices = np.argsort(importances)[::-1]

# Print the feature ranking
print("Feature ranking:")

for f in range(xtrain.shape[1]):
    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))

# Plot the feature importances of the forest
plt.figure()
plt.title("Feature importances")
plt.bar(range(xtrain.shape[1]), importances[indices],#list(xtrain)
       color="g", yerr=std[indices], align="center")
plt.xticks(range(xtrain.shape[1]), indices)
plt.xlim([-1, xtrain.shape[1]])
plt.show()
predictedrfr=rfr.predict(xtest)
print("Cross-validation scores for Random Forest Regressor are: ", scores)
#Random Forest with "genre" and "acousticness" as they came out to be the most important features
from sklearn.ensemble import RandomForestRegressor
rf1=RandomForestRegressor(n_estimators=100,min_samples_leaf=25)
xtrain1= xtrain.iloc[:, 0:4].drop(['artist_name','track_name'],axis=1)
xtest1=xtest.iloc[:,0:4].drop(['artist_name','track_name'],axis=1)
scores = cross_val_score(rf1, xtrain1, ytrain, cv=10)
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
rfr1=rf1.fit(xtrain1,ytrain)
predictedrfr1=rfr1.predict(xtest1)
rfr1.score
